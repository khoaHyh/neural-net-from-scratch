{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2e7556",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.typing as npt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2daa11-7792-49ae-9f9d-01238a425ea5",
   "metadata": {},
   "source": [
    "btw, \"wrt\" stands for \"with respect to\"\n",
    "\n",
    "#### network architecture\n",
    "- 2 layers (1 hidden + 1 ouput)\n",
    "- 1 neuron per layer (single weight/bias for each)\n",
    "- parameter init with `np.random.randn()`\n",
    "\n",
    "#### Objectives for part2.py\n",
    "[x] Implement ReLU activation function\n",
    "[x] Generate non-linear data\n",
    "[x] Define network architecture (# of layres, # of neurons, init weights)\n",
    "[x] Extend forward pass to handle multiple layers\n",
    "[x] Implement Backward passm\n",
    "[x] Update training loop\n",
    "7. Visualization and Experiement with Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4db14f-9e1b-4536-8746-a75baf26792f",
   "metadata": {},
   "source": [
    "#### Objectives for part2 jupyter notebook visualizations\n",
    "- **Data and Model Fit**: Show the non-linear data points and your model's predictions vs. what a linear model would predict\n",
    "- **Loss Comparison**: Compare loss curves between your 2-layer network and a simple linear model\n",
    "- **Hidden Layer Activations**: Visualize what the hidden layer learns (before and after ReLU)\n",
    "- **Decision Boundary Evolution**: Show how the model's predictions change during training\n",
    "- **Parameter Evolution**: Track how all 4 parameters (2 weights, 2 biases) evolve during training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6801a290-f1e9-4963-8261-f07b18863ea3",
   "metadata": {},
   "source": [
    "## 1. Generate Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5964ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)  # For reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f17af5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 100\n",
    "noise_level = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758d044d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate x values\n",
    "X = np.random.uniform(0, 10, n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e46fe7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate y values with some noise to mimic \"real world\" phenomenon\n",
    "# Non-linear relationship: quadratic function\n",
    "y = 0.5 * X**2 - 2 * X + 5 + np.random.normal(0, noise_level, n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79ed1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training and testing sets (80% train, 20% test)\n",
    "split_idx = int(0.8 * n_samples)\n",
    "X_train, X_test = X[:split_idx], X[split_idx:]\n",
    "y_train, y_test = y[:split_idx], y[split_idx:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5615e195-c39f-4e1f-8a22-359b478b88e4",
   "metadata": {},
   "source": [
    "## 2. Initialize parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe51d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight, weight2 = np.random.randn(), np.random.randn()  # coefficient/slope\n",
    "bias, bias2 = np.random.randn(), np.random.randn()  # intercept"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470df7b2-7512-469c-bd99-029fa278fa47",
   "metadata": {},
   "source": [
    "## 3. Define model functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d52e2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU(value: npt.NDArray[np.float64]) -> npt.NDArray[np.float64]:\n",
    "    return np.maximum(0, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36949561",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU_derivative(value: npt.NDArray[np.float64]) -> npt.NDArray[np.float64]:\n",
    "    return (value > 0).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb80f847",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(\n",
    "    x: npt.NDArray[np.float64], w: float, b: float, w2: float, b2: float\n",
    ") -> tuple[npt.NDArray[np.float64], npt.NDArray[np.float64], npt.NDArray[np.float64]]:\n",
    "    hidden_layer_raw_output = x * w + b\n",
    "    hidden_layer_activated_output = ReLU(hidden_layer_raw_output)\n",
    "    final_predictions = hidden_layer_activated_output * w2 + b2\n",
    "    # return intermediate valeus for backprop\n",
    "    return hidden_layer_raw_output, hidden_layer_activated_output, final_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a438c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_calculation(\n",
    "    y_pred: npt.NDArray[np.float64], y_true: npt.NDArray[np.float64]\n",
    ") -> np.floating:\n",
    "    return np.mean((y_pred - y_true) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81e6f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_gradients(\n",
    "    x: npt.NDArray[np.float64],\n",
    "    y: npt.NDArray[np.float64],\n",
    "    y_pred: npt.NDArray[np.float64],\n",
    "    hidden_layer_raw: npt.NDArray[np.float64],\n",
    "    hidden_layer_activated: npt.NDArray[np.float64],\n",
    "    output_weight: float,\n",
    ") -> dict[str, float]:\n",
    "    # Calculate all gradients via backpropagation\n",
    "    num_inputs = x.size\n",
    "\n",
    "    # Output layer gradients (calculated direct from loss func)\n",
    "    output_weight_gradient = (\n",
    "        -2 / num_inputs * np.sum(hidden_layer_activated * (y - y_pred))\n",
    "    )\n",
    "    output_bias_gradient = -2 / num_inputs * np.sum(y - y_pred)\n",
    "\n",
    "    # Hidden layer gradients (using chain rule)\n",
    "    loss_gradient_wrt_predictions = -2 * (y - y_pred) / num_inputs\n",
    "    hidden_activation_gradient = ReLU_derivative(hidden_layer_raw)\n",
    "\n",
    "    loss_gradient_wrt_hidden_activated = loss_gradient_wrt_predictions * output_weight\n",
    "    loss_gradient_wrt_hidden_raw = (\n",
    "        loss_gradient_wrt_hidden_activated * hidden_activation_gradient\n",
    "    )\n",
    "\n",
    "    hidden_weight_gradient = np.sum(loss_gradient_wrt_hidden_raw * x)\n",
    "    hidden_bias_gradient = np.sum(loss_gradient_wrt_hidden_raw)\n",
    "\n",
    "    return {\n",
    "        \"hidden_weight\": hidden_weight_gradient,\n",
    "        \"hidden_bias\": hidden_bias_gradient,\n",
    "        \"output_weight\": output_weight_gradient,\n",
    "        \"output_bias\": output_bias_gradient,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e9eac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(\n",
    "    hidden_weight: float,\n",
    "    hidden_bias: float,\n",
    "    output_weight: float,\n",
    "    output_bias: float,\n",
    "    gradients: dict[str, float],\n",
    "    learning_rate: float,\n",
    ") -> tuple[float, float, float, float]:\n",
    "    \"\"\"Update all parameters using calculated gradients\"\"\"\n",
    "    updated_hidden_weight = hidden_weight - learning_rate * gradients[\"hidden_weight\"]\n",
    "    updated_hidden_bias = hidden_bias - learning_rate * gradients[\"hidden_bias\"]\n",
    "    updated_output_weight = output_weight - learning_rate * gradients[\"output_weight\"]\n",
    "    updated_output_bias = output_bias - learning_rate * gradients[\"output_bias\"]\n",
    "\n",
    "    return (\n",
    "        updated_hidden_weight,\n",
    "        updated_hidden_bias,\n",
    "        updated_output_weight,\n",
    "        updated_output_bias,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20778e8c-2111-469c-8254-72e6913dd576",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9a308d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 0.01\n",
    "iterations = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c746445b",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe2286c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(iterations):\n",
    "    hidden_layer_raw, hidden_layer_activated, y_pred = forward_pass(\n",
    "        X_train, weight, bias, weight2, bias2\n",
    "    )\n",
    "    loss = loss_calculation(y_pred, y_train)\n",
    "    loss_history.append(loss)\n",
    "\n",
    "    gradients = calculate_gradients(\n",
    "        X_train, y_train, y_pred, hidden_layer_raw, hidden_layer_activated, weight2\n",
    "    )\n",
    "\n",
    "    weight, bias, weight2, bias2 = update_parameters(\n",
    "        weight, bias, weight2, bias2, gradients, learning_rate\n",
    "    )\n",
    "\n",
    "    if (i + 1) % 100 == 0:\n",
    "        print(\n",
    "            f\"Iteration {i + 1}/{iterations}, Loss: {loss:.4f}, Weight: {weight:.4f}, Bias: {bias:.4f}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc289179",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Final parameters: Weight = {weight:.4f}, Bias = {bias:.4f}\")\n",
    "print(f\"Final output layer: Weight2 = {weight2:.4f}, Bias2 = {bias2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff35288f-ced5-4243-8cd0-e37a50dd3599",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786b823a-6c81-4220-967b-47e8c8191779",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_line = np.linspace(min(X), max(X), 100)\n",
    "\n",
    "# Calculate y values for the model's line using final params\n",
    "y_model = weight * x_line + bias\n",
    "\n",
    "# Calculate y values for the true line\n",
    "y_true = true_slope * x_line + true_intercept\n",
    "\n",
    "# Plot data points\n",
    "plt.scatter(X_train, y_train, marker='^', color='blue', alpha=0.7, label='Training Data')\n",
    "plt.scatter(X_test, y_test, color='green', alpha=0.7, label='Test Data')\n",
    "\n",
    "# Plot regression lines\n",
    "plt.plot(x_line, y_model, 'r-', linewidth=2, label=f'Model: y = {weight:.2f}x + {bias:.2f}')\n",
    "plt.plot(x_line, y_true, 'k--', linewidth=2, label=f'True: y = {true_slope:.2f}x + {true_intercept:.2f}')\n",
    "\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.title('Linear Regression Results')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
