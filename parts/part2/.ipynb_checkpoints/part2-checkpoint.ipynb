{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2e7556",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.typing as npt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f867dff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "btw, \"wrt\" stands for \"with respect to\"\n",
    "\n",
    "[network architecture]\n",
    "- 2 layers (1 hidden + 1 ouput)\n",
    "- 1 neuron per layer (single weight/bias for each)\n",
    "- parameter init with `np.random.randn()`\n",
    "\n",
    "[Objectives]\n",
    "[x] Implement ReLU activation function\n",
    "[x] Generate non-linear data\n",
    "[x] Define network architecture (# of layres, # of neurons, init weights)\n",
    "[x] Extend forward pass to handle multiple layers\n",
    "[x] Implement Backward pass\n",
    "[x] Update training loop\n",
    "7. Visualization and Experiement with Hyperparameters\n",
    "NOTE: Part 7 is done in the corresponding jupyter notebook in the current directory.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5964ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Generate sample data\n",
    "np.random.seed(42)  # For reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f17af5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 100\n",
    "noise_level = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758d044d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate x values\n",
    "X = np.random.uniform(0, 10, n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e46fe7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate y values with some noise to mimic \"real world\" phenomenon\n",
    "# Non-linear relationship: quadratic function\n",
    "y = 0.5 * X**2 - 2 * X + 5 + np.random.normal(0, noise_level, n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79ed1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training and testing sets (80% train, 20% test)\n",
    "split_idx = int(0.8 * n_samples)\n",
    "X_train, X_test = X[:split_idx], X[split_idx:]\n",
    "y_train, y_test = y[:split_idx], y[split_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe51d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Initialize parameters. In training, these random values will be adjusted through gradient descent\n",
    "weight, weight2 = np.random.randn(), np.random.randn()  # coefficient/slope\n",
    "bias, bias2 = np.random.randn(), np.random.randn()  # intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d52e2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Define model functions\n",
    "def ReLU(value: npt.NDArray[np.float64]) -> npt.NDArray[np.float64]:\n",
    "    return np.maximum(0, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36949561",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU_derivative(value: npt.NDArray[np.float64]) -> npt.NDArray[np.float64]:\n",
    "    return (value > 0).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb80f847",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(\n",
    "    x: npt.NDArray[np.float64], w: float, b: float, w2: float, b2: float\n",
    ") -> tuple[npt.NDArray[np.float64], npt.NDArray[np.float64], npt.NDArray[np.float64]]:\n",
    "    hidden_layer_raw_output = x * w + b\n",
    "    hidden_layer_activated_output = ReLU(hidden_layer_raw_output)\n",
    "    final_predictions = hidden_layer_activated_output * w2 + b2\n",
    "    # return intermediate valeus for backprop\n",
    "    return hidden_layer_raw_output, hidden_layer_activated_output, final_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a438c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_calculation(\n",
    "    y_pred: npt.NDArray[np.float64], y_true: npt.NDArray[np.float64]\n",
    ") -> np.floating:\n",
    "    return np.mean((y_pred - y_true) ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81e6f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_gradients(\n",
    "    x: npt.NDArray[np.float64],\n",
    "    y: npt.NDArray[np.float64],\n",
    "    y_pred: npt.NDArray[np.float64],\n",
    "    hidden_layer_raw: npt.NDArray[np.float64],\n",
    "    hidden_layer_activated: npt.NDArray[np.float64],\n",
    "    output_weight: float,\n",
    ") -> dict[str, float]:\n",
    "    # Calculate all gradients via backpropagation\n",
    "    num_inputs = x.size\n",
    "\n",
    "    # Output layer gradients (calculated direct from loss func)\n",
    "    output_weight_gradient = (\n",
    "        -2 / num_inputs * np.sum(hidden_layer_activated * (y - y_pred))\n",
    "    )\n",
    "    output_bias_gradient = -2 / num_inputs * np.sum(y - y_pred)\n",
    "\n",
    "    # Hidden layer gradients (using chain rule)\n",
    "    loss_gradient_wrt_predictions = -2 * (y - y_pred) / num_inputs\n",
    "    hidden_activation_gradient = ReLU_derivative(hidden_layer_raw)\n",
    "\n",
    "    loss_gradient_wrt_hidden_activated = loss_gradient_wrt_predictions * output_weight\n",
    "    loss_gradient_wrt_hidden_raw = (\n",
    "        loss_gradient_wrt_hidden_activated * hidden_activation_gradient\n",
    "    )\n",
    "\n",
    "    hidden_weight_gradient = np.sum(loss_gradient_wrt_hidden_raw * x)\n",
    "    hidden_bias_gradient = np.sum(loss_gradient_wrt_hidden_raw)\n",
    "\n",
    "    return {\n",
    "        \"hidden_weight\": hidden_weight_gradient,\n",
    "        \"hidden_bias\": hidden_bias_gradient,\n",
    "        \"output_weight\": output_weight_gradient,\n",
    "        \"output_bias\": output_bias_gradient,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e9eac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(\n",
    "    hidden_weight: float,\n",
    "    hidden_bias: float,\n",
    "    output_weight: float,\n",
    "    output_bias: float,\n",
    "    gradients: dict[str, float],\n",
    "    learning_rate: float,\n",
    ") -> tuple[float, float, float, float]:\n",
    "    \"\"\"Update all parameters using calculated gradients\"\"\"\n",
    "    updated_hidden_weight = hidden_weight - learning_rate * gradients[\"hidden_weight\"]\n",
    "    updated_hidden_bias = hidden_bias - learning_rate * gradients[\"hidden_bias\"]\n",
    "    updated_output_weight = output_weight - learning_rate * gradients[\"output_weight\"]\n",
    "    updated_output_bias = output_bias - learning_rate * gradients[\"output_bias\"]\n",
    "\n",
    "    return (\n",
    "        updated_hidden_weight,\n",
    "        updated_hidden_bias,\n",
    "        updated_output_weight,\n",
    "        updated_output_bias,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9a308d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Training loop\n",
    "# Hyperparameters\n",
    "learning_rate = 0.01\n",
    "iterations = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c746445b",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe2286c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(iterations):\n",
    "    hidden_layer_raw, hidden_layer_activated, y_pred = forward_pass(\n",
    "        X_train, weight, bias, weight2, bias2\n",
    "    )\n",
    "    loss = loss_calculation(y_pred, y_train)\n",
    "    loss_history.append(loss)\n",
    "\n",
    "    gradients = calculate_gradients(\n",
    "        X_train, y_train, y_pred, hidden_layer_raw, hidden_layer_activated, weight2\n",
    "    )\n",
    "\n",
    "    weight, bias, weight2, bias2 = update_parameters(\n",
    "        weight, bias, weight2, bias2, gradients, learning_rate\n",
    "    )\n",
    "\n",
    "    if (i + 1) % 100 == 0:\n",
    "        print(\n",
    "            f\"Iteration {i + 1}/{iterations}, Loss: {loss:.4f}, Weight: {weight:.4f}, Bias: {bias:.4f}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc289179",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Final parameters: Weight = {weight:.4f}, Bias = {bias:.4f}\")\n",
    "print(f\"Final output layer: Weight2 = {weight2:.4f}, Bias2 = {bias2:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
